<h1>pubs2016.bib</h1><a name="moffat16Dereverberation"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#moffat16Dereverberation">moffat16Dereverberation</a>,
  title = {Implementation and Assessment of Joint Source Separation and Dereverberation},
  author = {Moffat, David and Reiss, Joshua D.},
  booktitle = {Proc. Audio Engineering Society Conference: 60th International Conference: DREAMS (Dereverberation and Reverberation of Audio, Music, and Speech)},
  year = {2016},
  organization = {Audio Engineering Society},
  date-added = {2016-04-07 15:12:05 +0000},
  date-modified = {2016-04-07 15:12:05 +0000}
}
</pre>

<a name="jillings16WebAudio"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#jillings16WebAudio">jillings16WebAudio</a>,
  title = {Web Audio Evaluation Tool: A framework for subjective assessment of audio},
  author = {Jillings, Nicholas and De Man, Brecht and Moffat, David and Reiss, Joshua D.},
  booktitle = {Proc. 2nd Web Audio Conference},
  year = {2016},
  month = {April},
  organization = {Web Audio Conference},
  date-added = {2016-04-07 15:12:02 +0000},
  date-modified = {2016-04-07 15:12:02 +0000},
  venue = {Georgia Tech, Atlanta, USA}
}
</pre>

<a name="mengal16Weapon"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#mengal16Weapon">mengal16Weapon</a>,
  title = {Modal Synthesis of Weapon Sounds},
  author = {Mengual, Lucas and Moffat, David and Reiss, Joshua D.},
  booktitle = {Proc. Audio Engineering Society Conference: 61st International Conference: Audio for Games},
  year = {2016},
  organization = {Audio Engineering Society},
  date-added = {2016-04-07 15:12:02 +0000},
  date-modified = {2016-04-07 15:12:02 +0000}
}
</pre>

<a name="turchet16Footstep"></a><pre>
@article{<a href="pubs2016_raw.html#turchet16Footstep">turchet16Footstep</a>,
  title = {What do your footsteps sound like? An investigation on interactive footstep sounds adjustment.},
  author = {Turchet, Luca and Moffat, David and Tajadura-Jim\'{e}nez, Ana and Reiss, Joshua D. and Stockman, Tony},
  journal = {Applied Acoustics},
  year = {2016},
  note = {"In Press"},
  date-added = {2016-04-07 15:11:58 +0000},
  date-modified = {2016-04-07 15:11:58 +0000}
}
</pre>

<a name="Gingras2016"></a><pre>
@article{<a href="pubs2016_raw.html#Gingras2016">Gingras2016</a>,
  title = {Linking melodic expectation to expressive performance timing and perceived musical tension.},
  author = {Gingras, B and Pearce, MT and Goodchild, M and Dean, RT and Wiggins, G and McAdams, S},
  journal = {Journal of experimental psychology. Human perception and performance},
  year = {2016},
  month = {Apr},
  pages = {594--609},
  volume = {42},
  abstract = {This research explored the relations between the predictability of musical structure, expressive timing in performance, and listeners' perceived musical tension. Studies analyzing the influence of expressive timing on listeners' affective responses have been constrained by the fact that, in most pieces, the notated durations limit performers' interpretive freedom. To circumvent this issue, we focused on the unmeasured prelude, a semi-improvisatory genre without notated durations. In Experiment 1, 12 professional harpsichordists recorded an unmeasured prelude on a harpsichord equipped with a MIDI console. Melodic expectation was assessed using a probabilistic model (IDyOM [Information Dynamics of Music]) whose expectations have been previously shown to match closely those of human listeners. Performance timing information was extracted from the MIDI data using a score-performance matching algorithm. Time-series analyses showed that, in a piece with unspecified note durations, the predictability of melodic structure measurably influenced tempo fluctuations in performance. In Experiment 2, another 10 harpsichordists, 20 nonharpsichordist musicians, and 20 nonmusicians listened to the recordings from Experiment 1 and rated the perceived tension continuously. Granger causality analyses were conducted to investigate predictive relations among melodic expectation, expressive timing, and perceived tension. Although melodic expectation, as modeled by IDyOM, modestly predicted perceived tension for all participant groups, neither of its components, information content or entropy, was Granger causal. In contrast, expressive timing was a strong predictor and was Granger causal. However, because melodic expectation was also predictive of expressive timing, our results outline a complete chain of influence from predictability of melodic structure via expressive performance timing to perceived musical tension. (PsycINFO Database Record},
  address = {Institute of Psychology, University of Innsbruck.},
  bdsk-url-1 = {<a href="http://dx.doi.org/10.1037/xhp0000141">http://dx.doi.org/10.1037/xhp0000141</a>},
  doi = {10.1037/xhp0000141},
  eissn = {1939-1277},
  howpublished = {Print-Electronic},
  issn = {0096-1523},
  issue = {4},
  language = {eng},
  owner = {dan},
  timestamp = {2016.04.04}
}
</pre>

<a name="Pearce2016"></a><pre>
@article{<a href="pubs2016_raw.html#Pearce2016">Pearce2016</a>,
  title = {Neuroaesthetics: The Cognitive Neuroscience of Aesthetic Experience.},
  author = {Pearce, MT and Zaidel, DW and Vartanian, O and Skov, M and Leder, H and Chatterjee, A and Nadal, M},
  journal = {Perspectives on psychological science : a journal of the Association for Psychological Science},
  year = {2016},
  month = {Mar},
  pages = {265--279},
  volume = {11},
  abstract = {The field of neuroaesthetics has gained in popularity in recent years but also attracted criticism from the perspectives both of the humanities and the sciences. In an effort to consolidate research in the field, we characterize neuroaesthetics as the cognitive neuroscience of aesthetic experience, drawing on long traditions of research in empirical aesthetics on the one hand and cognitive neuroscience on the other. We clarify the aims and scope of the field, identifying relations among neuroscientific investigations of aesthetics, beauty, and art. The approach we advocate takes as its object of study a wide spectrum of aesthetic experiences, resulting from interactions of individuals, sensory stimuli, and context. Drawing on its parent fields, a cognitive neuroscience of aesthetics would investigate the complex cognitive processes and functional networks of brain regions involved in those experiences without placing a value on them. Thus, the cognitive neuroscientific approach may develop in a way that is mutually complementary to approaches in the humanities.},
  address = {School of Electronic Engineering and Computer Science, Queen Mary University of London, England marcus.pearce@qmul.ac.uk.},
  bdsk-url-1 = {<a href="http://dx.doi.org/10.1177/1745691615621274">http://dx.doi.org/10.1177/1745691615621274</a>},
  doi = {10.1177/1745691615621274},
  eissn = {1745-6924},
  howpublished = {Print},
  issn = {1745-6916},
  issue = {2},
  language = {eng},
  owner = {dan},
  timestamp = {2016.04.04}
}
</pre>

<a name="Sigtia2016"></a><pre>
@article{<a href="pubs2016_raw.html#Sigtia2016">Sigtia2016</a>,
  title = {An End-to-End Neural Network for Polyphonic Piano Music Transcription},
  author = {Sigtia, S and BENETOS, E and Dixon, S},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year = {2016},
  month = {May},
  pages = {927--939},
  volume = {24},
  address = {Siddharth Sigtia, Queen Mary University of London, School of Electronic Engineering and Computer Science, Mile End Road, London, E1 4NS, United Kingdom},
  bdsk-url-1 = {<a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7416164">http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7416164</a>},
  bdsk-url-2 = {<a href="http://dx.doi.org/10.1109/TASLP.2016.2533858">http://dx.doi.org/10.1109/TASLP.2016.2533858</a>},
  day = {1},
  doi = {10.1109/TASLP.2016.2533858},
  issn = {2329-9290},
  issue = {5},
  keyword = {Automatic Music Transcription},
  owner = {dan},
  publicationstatus = {published},
  publisher = {IEEE},
  timestamp = {2016.04.04},
  url = {<a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7416164">http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7416164</a>}
}
</pre>

<a name="THALMANN"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#THALMANN">THALMANN</a>,
  title = {The Mobile Audio Ontology: Experiencing Dynamic Music Objects on Mobile Devices},
  author = {THALMANN, FS and perez carillo and fazekas and wiggins and sandler},
  year = {2016},
  abstract = {This paper is about the Mobile Audio Ontology, a semantic audio framework for the design of novel music consumption experiences on mobile devices. The framework is based on the concept of the Dynamic Music Object which is an amalgamation of audio files, structural and analytical information extracted from the audio, and information about how it should be rendered in realtime. The Mobile Audio Ontology allows producers and distributors to specify a great variety of ways of playing back music in controlled indeterministic as well as adaptive and interactive ways. Users can map mobile sensor data, user interface controls, or autonomous control units hidden from the listener to any musical parameter exposed in the definition of a Dynamic Music Object. These mappings can also be made dependent on semantic and analytical information extracted from the audio.},
  conference = {Tenth IEEE International Conference on Semantic Computing},
  owner = {dan},
  publicationstatus = {published},
  timestamp = {2016.04.04}
}
</pre>

<a name="agres2016evaluationsystems"></a><pre>
@article{<a href="pubs2016_raw.html#agres2016evaluationsystems">agres2016evaluationsystems</a>,
  title = {Evaluation of musical creativity and musical metacreation systems},
  author = {Agres, K and Forth, J and Wiggins, GA},
  journal = {Computers in Entertainment},
  year = {2016},
  month = {Dec},
  volume = {14},
  abstract = {Â© 2016 ACM.The field of computational creativity, including musical metacreation, strives to develop artificial systems that are capable of demonstrating creative behavior or producing creative artefacts. But the claim of creativity is often assessed, subjectively only on the part of the researcher and not objectively at all. This article provides theoretical motivation for more systematic evaluation of musical metacreation and computationally creative systems and presents an overview of current methods used to assess human and machine creativity that may be adapted for this purpose. In order to highlight the need for a varied set of evaluation tools, a distinction is drawn among three types of creative systems: those that are purely generative, those that contain internal or external feedback, and those that are capable of reflection and self-reflection. To address the evaluation of each of these aspects, concrete examples of methods and techniques are suggested to help researchers (1) evaluate their systems' creative process and generated artefacts, and test their impact on the perceptual, cognitive, and affective states of the audience, and (2) build mechanisms for reflection into the creative system, including models of human perception and cognition, to endow creative systems with internal evaluative mechanisms to drive self-reflective processes. The first type of evaluation can be considered external to the creative system and may be employed by the researcher to both better understand the efficacy of their system and its impact and to incorporate feedback into the system. Here we take the stance that understanding human creativity can lend insight to computational approaches, and knowledge of how humans perceive creative systems and their output can be incorporated into artificial agents as feedback to provide a sense of how a creation will impact the audience. The second type centers around internal evaluation, in which the system is able to reason about its own behavior and generated output. We argue that creative behavior cannot occur without feedback and reflection by the creative/metacreative system itself. More rigorous empirical testing will allow computational and metacreative systems to become more creative by definition and can be used to demonstrate the impact and novelty of particular approaches.},
  day = {1},
  doi = {10.1145/2967506},
  eissn = {1544-3981},
  issn = {1544-3574},
  issue = {3},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="agres2016modelingmodels"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#agres2016modelingmodels">agres2016modelingmodels</a>,
  title = {Modeling metaphor perception with distributional semantics vector space models},
  author = {Agres, KR and McGregor, S and Rataj, K and Purver, M and Wiggins, GA},
  booktitle = {CEUR Workshop Proceedings},
  year = {2016},
  month = {Jan},
  volume = {1767},
  abstract = {Â© 2016, CEUR-WS. All rights reserved.In this paper, we present a novel application of a computational model of word meaning to capture human judgments of the linguistic properties of metaphoricity, familiarity, and meaningfulness. We present data gathered from human subjects regarding their ratings of these properties over a set of word pairs specifically designed to exhibit varying degrees of metaphoricity. We then investigate whether these properties can be measured in terms of geometric features of a model of distributional lexical semantics. We compare the performance of two models, our own Concept Discovery Model which dynamically constructs context-sensitive subspaces, and a state-of-the-art static distributional semantic model, and find that our dynamic model performs significantly better in its measurement of metaphoricity.},
  day = {1},
  issn = {1613-0073},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="sturm2016``horsesystems"></a><pre>
@article{<a href="pubs2016_raw.html#sturm2016``horsesystems">sturm2016``horsesystems</a>,
  title = {``Horse'' Inside: 
Seeking Causes of the Behaviours of Music Content Analysis Systems},
  author = {STURM, BLT},
  journal = {ACM Computers in Entertainment},
  year = {2016},
  month = {Dec},
  note = {OA Monitor Exercise},
  number = {39},
  volume = {9},
  abstract = {Building systems that possess the sensitivity and intelligence to 
identify and describe high-level attributes in music audio signals 
continues to be an elusive goal,
but one that surely has broad and deep implications 
for a wide variety of applications.
Hundreds of papers have so far been published toward this goal,
and great progress appears to have been made.
Some systems produce remarkable accuracies 
at recognising high-level semantic concepts, such as music style, genre and mood.
However, it might be that these numbers do not mean what they seem.
In this paper, we take a state-of-the-art music content analysis system
and investigate what causes it to achieve
exceptionally high performance in a benchmark music audio dataset.
We dissect the system to understand its operation,
determine its sensitivities and limitations, and predict the kinds of knowledge
it could and could not possess about music.
We perform a series of experiments 
to illuminate what the system has actually learned to do,
and to what extent it is performing the intended music listening task.
Our results demonstrate how the initial manifestation of 
music intelligence in this state-of-the-art can be deceptive.
Our work provides constructive directions toward 
developing music content analysis systems that 
can address the music information and creation needs of real-world users.},
  day = {16},
  doi = {10.1145/2967507},
  issn = {1544-3981},
  issue = {4},
  keyword = {deep learning},
  language = {English},
  publicationstatus = {published},
  publisher = {ACM},
  timestamp = {2017.04.05}
}
</pre>

<a name="rodriguezalgarra2016analysingmusic?"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#rodriguezalgarra2016analysingmusic?">rodriguezalgarra2016analysingmusic?</a>,
  title = {Analysing Scattering-Based Music Content Analysis Systems: Where's the Music?},
  author = {RODRIGUEZ ALGARRA, F and Sturm, BL and Maruri-Aguilar, H},
  booktitle = {17th International Society for Music Information Retrieval Conference (ISMIR 2016)},
  year = {2016},
  month = {Aug},
  organization = {New York, NY, USA},
  abstract = {Music content analysis (MCA) systems built using scattering transform features are reported quite successful in the GTZAN benchmark music dataset. In this paper, we seek to answer why. We first analyse the feature extraction and classification components of scattering-based MCA systems. This guides us to perform intervention experiments on three factors: train/test partition, classifier and recording spectrum. The partition intervention shows a decrease in the amount of reproduced ground truth by the resulting systems. We then replace the learning algorithm with a binary decision tree, and identify the impact of specific feature dimensions. We finally alter the spectral content related to such dimensions, which reveals that these scattering-based systems exploit acoustic information below 20 Hz to reproduce GTZAN ground truth. The source code to reproduce our experiments is available online.},
  day = {7},
  finishday = {11},
  finishmonth = {Aug},
  finishyear = {2016},
  publicationstatus = {accepted},
  startday = {7},
  startmonth = {Aug},
  startyear = {2016},
  timestamp = {2017.04.05}
}
</pre>

<a name="stowell2016detailedsongbirds"></a><pre>
@article{<a href="pubs2016_raw.html#stowell2016detailedsongbirds">stowell2016detailedsongbirds</a>,
  title = {Detailed temporal structure of communication networks in groups of songbirds},
  author = {STOWELL, DF and Gill, LF and Clayton, D},
  journal = {Journal of the Royal Society Interface},
  year = {2016},
  month = {Jun},
  volume = {13},
  abstract = {Animals in groups often exchange calls, in patterns whose temporal structure may be influenced by contextual factors such as physical location and the social network structure of the group. We introduce a model-based analysis for temporal patterns of animal call timing, originally developed for networks of firing neurons. This has advantages over cross-correlation analysis in that it can correctly handle common-cause confounds and provides a generative model of call patterns with explicit parameters for the influences between individuals. It also has advantages over standard Markovian analysis in that it incorporates detailed temporal interactions which affect timing as well as sequencing of calls. Further, a fitted model can be used to generate novel synthetic call sequences. We apply the method to calls recorded from groups of domesticated zebra finch (Taeniopygia guttata) individuals. We find that the communication network in these groups has stable structure that persists from one day to the next, and that âkernelsâ reflecting the temporal range of influence have a characteristic structure for a calling individualâs effect on itself, its partner, and on others in the group. We further find characteristic patterns of influences by call type as well as by individual.},
  day = {22},
  doi = {10.1098/rsif.2016.0296},
  issn = {1742-5689},
  issue = {119},
  publicationstatus = {published},
  publisher = {Royal Society, The},
  timestamp = {2017.04.05}
}
</pre>

<a name="stockman2016reflectionsseeking"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#stockman2016reflectionsseeking">stockman2016reflectionsseeking</a>,
  title = {Reflections on the Research Methods Used in an Investigation of Cross-modal Collaborative Information Seeking},
  author = {STOCKMAN, AG and Al-Thani, D},
  booktitle = {The 5th International Workshop onn Collaboration: Human-Centered Issues \& Interactivity Design, as part of The 2016 International Conference on Collaboration Technologies and Systems, Florida, USA, October-November 2016.},
  year = {2016},
  month = {Oct},
  organization = {Florida, USA},
  day = {31},
  finishday = {4},
  finishmonth = {Nov},
  finishyear = {2016},
  publicationstatus = {accepted},
  startday = {31},
  startmonth = {Oct},
  startyear = {2016},
  timestamp = {2017.04.05}
}
</pre>

<a name="metatla2016sonificationtasks"></a><pre>
@article{<a href="pubs2016_raw.html#metatla2016sonificationtasks">metatla2016sonificationtasks</a>,
  title = {Sonification of reference markers for auditory graphs: effects on non-visual estimation tasks},
  author = {Metatla, O and Bryan-Kinns, N and Stockman, T and Martin, F},
  journal = {PeerJ Computer Science},
  year = {2016},
  month = {Apr},
  volume = {2},
  day = {6},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="tian2016towardsprinciples"></a><pre>
@article{<a href="pubs2016_raw.html#tian2016towardsprinciples">tian2016towardsprinciples</a>,
  title = {Towards Music Structural Segmentation Across Genres: Features, Structural Hypotheses and Annotation Principles},
  author = {TIAN, M and Sandler, MARKB},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  year = {2016},
  month = {Nov},
  day = {2},
  issn = {2157-6912},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="choi2016text-basedcomposition"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#choi2016text-basedcomposition">choi2016text-basedcomposition</a>,
  title = {Text-based LSTM networks for Automatic Music Composition},
  author = {CHOI, K and sandler, M and fazekas, G},
  booktitle = {Conference on Computer Simulation of Musical Creativity},
  year = {2016},
  address = {Keunwoo Choi, Queen Mary University of London, EECS, Peter Landin Building, CS.319, London, E1 4FZ, United Kingdom},
  month = {Jun},
  organization = {Huddersfield, UK},
  abstract = {In this paper, we introduce new methods and discuss results of text-based LSTM (Long Short-Term Memory) networks for automatic music composition. The proposed network is designed to learn relationships within text documents that represent chord progressions and drum tracks in two case studies. In the experiments, word-RNNs (Recurrent Neural Networks) show good results for both cases, while character-based RNNs (char-RNNs) only succeed to learn chord progressions. The proposed system can be used for fully automatic composition or as semi-automatic systems that help humans to compose music by controlling a diversity parameter of the model.},
  day = {18},
  finishday = {19},
  finishmonth = {Jun},
  finishyear = {2016},
  keyword = {automatic composition},
  publicationstatus = {accepted},
  startday = {17},
  startmonth = {Jun},
  startyear = {2016},
  timestamp = {2017.04.05}
}
</pre>

<a name="allik2016ontologicalfeatures"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#allik2016ontologicalfeatures">allik2016ontologicalfeatures</a>,
  title = {Ontological Representation of Audio Features},
  author = {Allik, A and Fazekas, G and Sandler, M},
  booktitle = {SEMANTIC WEB - ISWC 2016, PT II},
  year = {2016},
  pages = {3--11},
  volume = {9982},
  doi = {10.1007/978-3-319-46547-0_1},
  isbn = {978-3-319-46546-3},
  issn = {0302-9743},
  keyword = {Semantic audio analysis},
  publicationstatus = {published},
  timestamp = {2017.04.05},
  url = {<a href="http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&amp;SrcApp=PARTNER_APP\&amp;SrcAuth=LinksAMR\&amp;KeyUT=WOS:000389086600001\&amp;DestLinkType=FullRecord\&amp;DestApp=ALL_WOS\&amp;UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a">http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&SrcApp=PARTNER_APP\&SrcAuth=LinksAMR\&KeyUT=WOS:000389086600001\&DestLinkType=FullRecord\&DestApp=ALL_WOS\&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a</a>}
}
</pre>

<a name="ohanlon2016compositionaldistance"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#ohanlon2016compositionaldistance">ohanlon2016compositionaldistance</a>,
  title = {COMPOSITIONAL CHROMA ESTIMATION USING POWERED EUCLIDEAN DISTANCE},
  author = {O'Hanlon, K and Sandler, MB and IEEE},
  booktitle = {2016 24TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO)},
  year = {2016},
  pages = {1237--1241},
  issn = {2076-1465},
  keyword = {Compositional model},
  publicationstatus = {published},
  timestamp = {2017.04.05},
  url = {<a href="http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&amp;SrcApp=PARTNER_APP\&amp;SrcAuth=LinksAMR\&amp;KeyUT=WOS:000391891900236\&amp;DestLinkType=FullRecord\&amp;DestApp=ALL_WOS\&amp;UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a">http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&SrcApp=PARTNER_APP\&SrcAuth=LinksAMR\&KeyUT=WOS:000391891900236\&DestLinkType=FullRecord\&DestApp=ALL_WOS\&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a</a>}
}
</pre>

<a name="wilmering2016aufx-o:workflows"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#wilmering2016aufx-o:workflows">wilmering2016aufx-o:workflows</a>,
  title = {AUFX-O: Novel methods for the representation of audio processing workflows},
  author = {Wilmering, T and Fazekas, G and Sandler, MB},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year = {2016},
  month = {Jan},
  pages = {229--237},
  volume = {9982 LNCS},
  abstract = {Â© Springer International Publishing AG 2016.This paper introduces the Audio Effect Ontology (AUFX-O) building on previous theoretical models describing audio processing units and workflows in the context of music production. We discuss important conceptualisations of different abstraction layers, their necessity to successfully model audio effects, and their application method. We present use cases concerning the use of effects in music production projects and the creation of audio effect metadata facilitating a linked data service exposing information about effect implementations. By doing so, we show how our model facilitates knowledge sharing, reproducibility and analysis of audio production workflows.},
  day = {1},
  doi = {10.1007/978-3-319-46547-0_24},
  eissn = {1611-3349},
  isbn = {9783319465463},
  issn = {0302-9743},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="ohanlon2016annmf"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#ohanlon2016annmf">ohanlon2016annmf</a>,
  title = {AN ITERATIVE HARD THRESHOLDING APPROACH TO l(0) SPARSE HELLINGER NMF},
  author = {O'Hanlon, K and Sandler, MB and IEEE},
  booktitle = {2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING PROCEEDINGS},
  year = {2016},
  pages = {4737--4741},
  issn = {1520-6149},
  keyword = {Non-negative matrix factoriation},
  publicationstatus = {published},
  timestamp = {2017.04.05},
  url = {<a href="http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&amp;SrcApp=PARTNER_APP\&amp;SrcAuth=LinksAMR\&amp;KeyUT=WOS:000388373404177\&amp;DestLinkType=FullRecord\&amp;DestApp=ALL_WOS\&amp;UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a">http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&SrcApp=PARTNER_APP\&SrcAuth=LinksAMR\&KeyUT=WOS:000388373404177\&DestLinkType=FullRecord\&DestApp=ALL_WOS\&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a</a>}
}
</pre>

<a name="rodriguez-serrano2016arecordings"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#rodriguez-serrano2016arecordings">rodriguez-serrano2016arecordings</a>,
  title = {A Score-Informed Shift-Invariant Extension of Complex Matrix Factorization for Improving the Separation of Overlapped Partials in Music Recordings},
  author = {Rodriguez-Serrano, FJ and Ewert, S and Vera-Candeas, P and Sandler, M},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year = {2016},
  address = {Shanghai, China},
  timestamp = {2017.04.05}
}
</pre>

<a name="stables2016semanticproduction"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#stables2016semanticproduction">stables2016semanticproduction</a>,
  title = {Semantic description of timbral transformations in music production},
  author = {Stables, R and De Man, B and Enderby, S and Reiss, JD and Fazekas, G and Wilmering, T},
  booktitle = {MM 2016 - Proceedings of the 2016 ACM Multimedia Conference},
  year = {2016},
  month = {Oct},
  pages = {337--341},
  abstract = {Â© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.In music production, descriptive terminology is used to define perceived sound transformations. By understanding the underlying statistical features associated with these descriptions, we can aid the retrieval of contextually relevant processing parameters using natural language, and create intelligent systems capable of assisting in audio engineering. In this study, we present an analysis of a dataset containing descriptive terms gathered using a series of processing modules, embedded within a Digital Audio Workstation. By applying hierarchical clustering to the audio feature space, we show that similarity in term representations exists within and between transformation classes. Furthermore, the organisation of terms in low-dimensional timbre space can be explained using perceptual concepts such as size and dissonance. We conclude by performing Latent Semantic Indexing to show that similar groupings exist based on term frequency.},
  day = {1},
  doi = {10.1145/2964284.2967238},
  isbn = {9781450336031},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="deman2016thecases"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#deman2016thecases">deman2016thecases</a>,
  title = {The Open Multitrack Testbed: Features, content and use cases},
  author = {DE MAN, BM and Reiss, JD},
  booktitle = {<a href="http://c4dm.eecs.qmul.ac.uk/events/wimp2/">http://c4dm.eecs.qmul.ac.uk/events/wimp2/</a>},
  year = {2016},
  month = {Sep},
  organization = {Queen Mary University of London, London, UK},
  abstract = {The Open Multitrack Testbed is an online repository of multitrack audio accessible to the public, with rich metadata annotation, a semantic database and search functionality. Two years after it first went live, the dataset is the largest and most diverse available, and still growing. An overview of the available content, some prominent features, and example uses in the field of intelligent music production are dis- cussed.},
  day = {13},
  finishday = {13},
  finishmonth = {Sep},
  finishyear = {2016},
  keyword = {dataset},
  publicationstatus = {published},
  startday = {13},
  startmonth = {Sep},
  startyear = {2016},
  timestamp = {2017.04.05}
}
</pre>

<a name="deman2016subjectivetool"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#deman2016subjectivetool">deman2016subjectivetool</a>,
  title = {Subjective comparison of music production practices using the Web Audio Evaluation Tool},
  author = {DE MAN, BM and Jillings, N and Moffat, D and Reiss, JD and Stables, R},
  booktitle = {<a href="http://c4dm.eecs.qmul.ac.uk/events/wimp2/">http://c4dm.eecs.qmul.ac.uk/events/wimp2/</a>},
  year = {2016},
  month = {Sep},
  organization = {Queen Mary University of London, London, UK},
  abstract = {The Web Audio Evaluation Tool is an open-source, browser- based framework for creating and conducting listening tests. It allows remote deployment, GUI-guided setup, and analysis in the browser. While currently being used for listening tests in various fields, it was initially developed specifically for the study of music production practices. In this work, we highlight some of the features that facilitate evaluation of such content.},
  day = {13},
  finishday = {13},
  finishmonth = {Sep},
  finishyear = {2016},
  keyword = {perceptual evaluation},
  publicationstatus = {published},
  startday = {13},
  startmonth = {Sep},
  startyear = {2016},
  timestamp = {2017.04.05}
}
</pre>

<a name="reiss2016aevaluation"></a><pre>
@article{<a href="pubs2016_raw.html#reiss2016aevaluation">reiss2016aevaluation</a>,
  title = {A Meta-Analysis of High Resolution Audio Perceptual Evaluation},
  author = {REISS, J},
  journal = {Journal of the Audio Engineering Society},
  year = {2016},
  month = {Jun},
  pages = {364--379},
  volume = {64},
  abstract = {Over the last decade, there has been considerable debate over the benefits of recording and rendering high resolution audio beyond standard CD quality audio. This research involved a systematic review and meta-analysis (combining the results of numerous independent studies) to assess the ability of test subjects to perceive a difference between high resolution and standard (16 bit, 44.1 or 48 kHz) audio. Eighteen published experiments for which sufficient data could be obtained were included, providing a meta-analysis that combined over 400 participants in more than 12,500 trials. Results showed a small but statistically significant ability of test subjects to discriminate high resolution content, and this effect increased dramatically when test subjects received extensive training. This result was verified by a sensitivity analysis exploring different choices for the chosen studies and different analysis approaches. Potential biases in studies, effect of test methodology, experimental design, and choice of stimuli were also investigated. The overall conclusion is that the perceived fidelity of an audio recording and playback chain can be affected by operating beyond conventional resolution.},
  day = {27},
  doi = {10.17743/jaes.2016.0015},
  issn = {1549-4950},
  issue = {6},
  language = {English},
  publicationstatus = {published},
  publisher = {Audio Engineering Society},
  timestamp = {2017.04.05},
  url = {tp://www.aes.org/}
}
</pre>

<a name="wang2016anmicrophones"></a><pre>
@article{<a href="pubs2016_raw.html#wang2016anmicrophones">wang2016anmicrophones</a>,
  title = {An Iterative Approach to Source Counting and Localization Using Two Distant Microphones},
  author = {Wang, L and Hon, TK and Reiss, JD and Cavallaro, A},
  journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
  year = {2016},
  month = {Jun},
  pages = {1079--1093},
  volume = {24},
  abstract = {Â© 2014 IEEE.We propose a time difference of arrival (TDOA) estimation framework based on time-frequency inter-channel phase difference (IPD) to count and localize multiple acoustic sources in a reverberant environment using two distant microphones. The time-frequency (T-F) processing enables exploitation of the nonstationarity and sparsity of audio signals, increasing robustness to multiple sources and ambient noise. For inter-channel phase difference estimation, we use a cost function, which is equivalent to the generalized cross correlation with phase transform (GCC) algorithm and which is robust to spatial aliasing caused by large inter-microphone distances. To estimate the number of sources, we further propose an iterative contribution removal (ICR) algorithm to count and locate the sources using the peaks of the GCC function. In each iteration, we first use IPD to calculate the GCC function, whose highest peak is detected as the location of a sound source; then we detect the T-F bins that are associated with this source and remove them from the IPD set. The proposed ICR algorithm successfully solves the GCC peak ambiguities between multiple sources and multiple reverberant paths.},
  day = {1},
  doi = {10.1109/TASLP.2016.2533859},
  issn = {2329-9290},
  issue = {6},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="vlimki2016allfrontiers"></a><pre>
@article{<a href="pubs2016_raw.html#vlimki2016allfrontiers">vlimki2016allfrontiers</a>,
  title = {All About Audio Equalization: Solutions and Frontiers},
  author = {VÃ¤limÃ¤ki, V and Reiss, J},
  journal = {Applied Sciences},
  year = {2016},
  month = {May},
  pages = {129--129},
  volume = {6},
  day = {6},
  doi = {10.3390/app6050129},
  eissn = {2076-3417},
  issn = {2076-3417},
  issue = {5},
  publicationstatus = {published},
  publisher = {MDPI},
  timestamp = {2017.04.05}
}
</pre>

<a name="jillings2016webaudio"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#jillings2016webaudio">jillings2016webaudio</a>,
  title = {Web Audio Evaluation Tool: A framework for subjective assessment of audio},
  author = {Jillings, N and DE MAN, BM and Moffat, D and Reiss, JD and Stables, R},
  booktitle = {https://smartech.gatech.edu/handle/1853/54577/browse},
  year = {2016},
  editor = {Freeman, J and Lerch, A and Paradis, M},
  month = {Apr},
  organization = {Atlanta, GA, USA},
  abstract = {Perceptual listening tests are commonplace in audio research and a vital form of evaluation. While a large number of tools exist to run such tests, many feature just one test type, are platform dependent, run on proprietary software, or require considerable configuration and programming. Using Web Audio, the Web Audio Evaluation Tool (WAET) addresses these concerns by having one toolbox which can be configured to run many different tests, perform it through a web browser and without needing proprietary software or computer programming knowledge. In this paper the role of the Web Audio API in giving WAET key functionalities
 are shown. The paper also highlights less common features, available to web based tools, such as easy remote testing environment and in-browser analytics.},
  day = {3},
  finishday = {6},
  finishmonth = {Apr},
  finishyear = {2016},
  publicationstatus = {published},
  startday = {4},
  startmonth = {Apr},
  startyear = {2016},
  timestamp = {2017.04.05},
  url = {https://github.com/BrechtDeMan/WebAudioEvaluationTool}
}
</pre>

<a name="wang2016self-localizationarrivals"></a><pre>
@article{<a href="pubs2016_raw.html#wang2016self-localizationarrivals">wang2016self-localizationarrivals</a>,
  title = {Self-localization of ad-hoc arrays using time difference of arrivals},
  author = {Wang, L and Hon, TK and Reiss, JD and Cavallaro, A},
  journal = {IEEE Transactions on Signal Processing},
  year = {2016},
  month = {Feb},
  pages = {1018--1033},
  volume = {64},
  abstract = {Â© 2015 IEEE.We investigate the problem of sensor and source joint localization using time-difference of arrivals (TDOAs) of an ad-hoc array. A major challenge is that the TDOAs contain unknown time offsets between asynchronous sensors. To address this problem, we propose a low-rank approximation method that does not need any prior knowledge of sensor and source locations or timing information. At first, we construct a pseudo time of arrival (TOA) matrix by introducing two sets of unknown timing parameters (source onset times and device capture times) into the current TDOA matrix. Then we propose a Gauss-Newton low-rank approximation algorithm to jointly identify the two sets of unknown timing parameters, exploiting the low-rank property embedded in the pseudo TOA matrix. We derive the boundaries of the timing parameters to reduce the initialization space and employ a multi-initialization scheme. Finally, we use the estimated timing parameters to correct the pseudo TOA matrix, which is further applied to sensor and source localization. Experimental results show that the proposed approach outperforms state-of-the-art algorithms.},
  day = {15},
  doi = {10.1109/TSP.2015.2498130},
  issn = {1053-587X},
  issue = {4},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="chourdakis2016automaticmodels"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#chourdakis2016automaticmodels">chourdakis2016automaticmodels</a>,
  title = {Automatic Control of a Digital Reverberation Effect using Hybrid Models},
  author = {CHOURDAKIS, ET and REISS, JD},
  booktitle = {60th International Conference: DREAMS (Dereverberation and Reverberation of Audio, Music, and Speech)},
  year = {2016},
  month = {Jan},
  organization = {Leuven, Belgium},
  abstract = {Adaptive Digital Audio Effects are sound transformations controlled by features extracted from the sound itself. Artificial reverberation is used by sound engineers in the mixing process for a variety of technical and artistic reasons, including to give the perception that it was captured in a closed space. We propose a design of an adaptive digital audio effect for artificial reverberation that allows it to learn from the user in a supervised way. We perform feature selection and dimensionality reduction on features extracted from our training data set. Then a user provides examples of reverberation parameters for the training data. Finally, we train a set of classifiers and compare them using 10-fold cross validation to compare classification success ratios and mean squared errors. Tracks from the Open Multitrack Testbed are used in order to train and test our models.},
  day = {27},
  finishday = {5},
  finishmonth = {Feb},
  finishyear = {2016},
  publicationstatus = {published},
  startday = {3},
  startmonth = {Feb},
  startyear = {2016},
  timestamp = {2017.04.05}
}
</pre>

<a name="forth2016entrainingthinking"></a><pre>
@article{<a href="pubs2016_raw.html#forth2016entrainingthinking">forth2016entrainingthinking</a>,
  title = {Entraining IDyOT: timing in the information dynamics of thinking},
  author = {FORTH, J and AGRES, K and PURVER, MRJ and WIGGINS, GA},
  journal = {Frontiers in Psychology},
  year = {2016},
  month = {Oct},
  pages = {1575--1575},
  volume = {7},
  abstract = {We present a novel hypothetical account of entrainment in music and language, in context of the Information Dynamics of Thinking model, IDyOT. The extended model affords an alternative view of entrainment, and its companion term, pulse, from earlier accounts. The model is based on hiearchical, statistical prediction, modeling expectations of both what an event will be and when it will happen. As such,it constitutes a kind of predictive coding, with a particular novel hypothetical implementation. Here, we focus on the modelâs mechanism for predicting when a perceptual event will happen, given an existing sequence of past events, which maybe musical or linguistic. We propose a range of tests to validate or falsify the model, at various different levels of abstraction, and argue that computational modelling in particular, and this model in particular, can offer a means of providing limited but useful evidence for evolutionary hypotheses.},
  day = {18},
  doi = {10.3389/fpsyg.2016.01575},
  editor = {RAVIGNANI, A},
  issn = {1664-1078},
  publicationstatus = {published},
  publisher = {Frontiers Media},
  timestamp = {2017.04.05},
  url = {<a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2016.01575/abstract">http://journal.frontiersin.org/article/10.3389/fpsyg.2016.01575/abstract</a>}
}
</pre>

<a name="hansen2016ifmusic"></a><pre>
@article{<a href="pubs2016_raw.html#hansen2016ifmusic">hansen2016ifmusic</a>,
  title = {"If You Have to Ask, You'll Never Know": Effects of Specialised Stylistic Expertise on Predictive Processing of Music},
  author = {Hansen, NC and Vuust, P and Pearce, M},
  journal = {PLOS ONE},
  year = {2016},
  month = {Oct},
  pages = {e0163584--e0163584},
  volume = {11},
  day = {12},
  doi = {10.1371/journal.pone.0163584},
  editor = {Jaencke, L},
  eissn = {1932-6203},
  issue = {10},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="dean2016algorithmically-generatedmusic"></a><pre>
@article{<a href="pubs2016_raw.html#dean2016algorithmically-generatedmusic">dean2016algorithmically-generatedmusic</a>,
  title = {Algorithmically-generated Corpora that use Serial Compositional Principles Can Contribute to the Modeling of Sequential Pitch Structure in Non-tonal Music},
  author = {Dean, RT and Pearce, MT},
  journal = {Empirical Musicology Review},
  year = {2016},
  month = {Jul},
  pages = {27--27},
  volume = {11},
  day = {8},
  doi = {10.18061/emr.v11i1.4900},
  issn = {1559-5749},
  issue = {1},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="hansen2016nonlinearmusicology"></a><pre>
@article{<a href="pubs2016_raw.html#hansen2016nonlinearmusicology">hansen2016nonlinearmusicology</a>,
  title = {Nonlinear Changes in the Rhythm of European Art Music: Quantitative Support for Historical Musicology},
  author = {Hansen, NC and Sadakata, M and Pearce, M},
  journal = {Music Perception: An Interdisciplinary Journal},
  year = {2016},
  month = {Apr},
  pages = {414--431},
  volume = {33},
  day = {1},
  doi = {10.1525/mp.2016.33.4.414},
  eissn = {1533-8312},
  issn = {0730-7829},
  issue = {4},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="barascud2016brainpatterns."></a><pre>
@article{<a href="pubs2016_raw.html#barascud2016brainpatterns.">barascud2016brainpatterns.</a>,
  title = {Brain responses in humans reveal ideal observer-like sensitivity to complex acoustic patterns.},
  author = {Barascud, N and Pearce, MT and Griffiths, TD and Friston, KJ and Chait, M},
  journal = {Proc Natl Acad Sci U S A},
  year = {2016},
  month = {Feb},
  pages = {E616--E625},
  volume = {113},
  abstract = {We use behavioral methods, magnetoencephalography, and functional MRI to investigate how human listeners discover temporal patterns and statistical regularities in complex sound sequences. Sensitivity to patterns is fundamental to sensory processing, in particular in the auditory system, because most auditory signals only have meaning as successions over time. Previous evidence suggests that the brain is tuned to the statistics of sensory stimulation. However, the process through which this arises has been elusive. We demonstrate that listeners are remarkably sensitive to the emergence of complex patterns within rapidly evolving sound sequences, performing on par with an ideal observer model. Brain responses reveal online processes of evidence accumulation--dynamic changes in tonic activity precisely correlate with the expected precision or predictability of ongoing auditory input--both in terms of deterministic (first-order) structure and the entropy of random sequences. Source analysis demonstrates an interaction between primary auditory cortex, hippocampus, and inferior frontal gyrus in the process of discovering the regularity within the ongoing sound sequence. The results are consistent with precision based predictive coding accounts of perceptual inference and provide compelling neurophysiological evidence of the brain's capacity to encode high-order temporal structure in sensory signals.},
  day = {2},
  doi = {10.1073/pnas.1508523113},
  eissn = {1091-6490},
  issue = {5},
  keyword = {MEG},
  language = {eng},
  organization = {United States},
  pii = {1508523113},
  publicationstatus = {published},
  timestamp = {2017.04.05},
  url = {https://www.ncbi.nlm.nih.gov/pubmed/26787854}
}
</pre>

<a name="pearce2016amusic"></a><pre>
@incollection{<a href="pubs2016_raw.html#pearce2016amusic">pearce2016amusic</a>,
  title = {A New Look at Musical Expectancy: The Veridical Versus the General in the Mental Organization of Music},
  author = {PEARCE, MT and Schubert, E},
  booktitle = {Music, Mind, and Embodiment},
  publisher = {Springer},
  year = {2016},
  editor = {Kronland-Martinet, R and Aramaki, M and Ystad, S},
  pages = {358--370},
  doi = {10.1007/978-3-319-46282-0_23},
  isbn = {978-3-319-46281-3},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="liang2016classificationsystem"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#liang2016classificationsystem">liang2016classificationsystem</a>,
  title = {Classification of Piano Pedaling Techniques Using Gesture Data from a Non-Intrusive Measurement System},
  author = {LIANG, B and fazekas, G and mcpherson, A and sandler, M},
  booktitle = {DMRN+11: Digital Music Research Network Workshop Proceedings 2016},
  year = {2016},
  month = {Dec},
  organization = {Centre for Digital Music, Queen Mary University of London},
  publisher = {Centre for Digital Music, Queen Mary University of London},
  abstract = {This paper presents the results of a study of piano pedaling techniques on the sustain pedal using a newly designed measurement system. This system is comprised of an optical sensor mounted in the pedal bearing block and the Bela platform for recording audio and sensor data. Using the gesture data collected from the system, the task of classifying these data by pedaling technique was undertaken using a Support Vector Machine (SVM)},
  day = {20},
  finishday = {20},
  finishmonth = {Dec},
  finishyear = {2016},
  publicationstatus = {published},
  startday = {20},
  startmonth = {Dec},
  startyear = {2016},
  timestamp = {2017.04.05},
  url = {https://qmro.qmul.ac.uk/xmlui/handle/123456789/19345}
}
</pre>

<a name="jack2016effectinstrument"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#jack2016effectinstrument">jack2016effectinstrument</a>,
  title = {Effect of latency on performer interaction and subjective quality assessment of a digital musical instrument},
  author = {Jack, RH and Stockman, T and McPherson, A},
  booktitle = {ACM International Conference Proceeding Series},
  year = {2016},
  month = {Oct},
  pages = {116--123},
  volume = {04-06-October-2016},
  abstract = {Â© 2016 Copyright held by the owner/author(s).When designing digital musical instruments the importance of low and consistent action-to-sound latency is widely accepted. This paper investigates the effects of latency (0-20ms) on instrument quality evaluation and performer interaction. We present findings from an experiment conducted with musicians who performed on an percussive digital musical instrument with variable amounts of latency. Three latency conditions were tested against a zero latency condition, 10ms, 20ms and 10ms Â± 3ms jitter. The zero latency condition was significantly rated more positively than the 10ms with jitter and 20ms latency conditions in six quality measures, emphasising the importance of not only low, but stable latency in digital musical instruments. There was no significant difference in rating between the zero latency condition and 10ms condition. A quantitative analysis of timing accuracy in a metronome task under latency conditions showed no significant difference in mean synchronisation error. This suggests that the 20ms and 10ms with jitter latency conditions degrade subjective impressions of an instrument, but without significantly affecting the timing performance of our participants. These findings are discussed in terms of control intimacy and instrument transparency.},
  day = {4},
  doi = {10.1145/2986416.2986428},
  isbn = {9781450348225},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="mcpherson2016action-soundenough?"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#mcpherson2016action-soundenough?">mcpherson2016action-soundenough?</a>,
  title = {Action-Sound Latency: Are Our Tools Fast Enough?},
  author = {McPherson, AP and Jack, RH and Moro, G},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression, Brisbane, Queensland, Australia, July 11-15, 2016},
  year = {2016},
  month = {Jul},
  note = {Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).},
  organization = {Griffith University, South Bank, Brisbane, Queensland, Australia},
  publisher = {Griffith University},
  abstract = {The importance of low and consistent latency in interactive music systems is well-established. So how do commonly-used tools for creating digital musical instruments and other tangible interfaces perform in terms of latency from user action to sound output? This paper examines several common configurations where a microcontroller (e.g. Arduino) or wireless device communicates with computer-based sound generator (e.g. Max/MSP, Pd). We find that, perhaps surprisingly, almost none of the tested configurations meet generally-accepted guidelines for latency and jitter. To address this limitation, the paper presents a new embedded platform, Bela, which is capable of complex audio and sensor processing at submillisecond latency.},
  day = {11},
  finishday = {15},
  finishmonth = {Jul},
  finishyear = {2016},
  keyword = {Latency},
  publicationstatus = {accepted},
  startday = {11},
  startmonth = {Jul},
  startyear = {2016},
  timestamp = {2017.04.05},
  url = {https://nime2016.wordpress.com/}
}
</pre>

<a name="moro2016makingdata"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#moro2016makingdata">moro2016makingdata</a>,
  title = {Making High-Performance Embedded Instruments with Bela and Pure Data},
  author = {MORO, G and Bin, A and Jack, RH and Heinrichs, C and McPherson, AP},
  booktitle = {International Conference on Live Interfaces},
  year = {2016},
  month = {Jun},
  note = {This hands-on workshop introduces participants to Bela, an embedded platform for ultra-low latency audio and sensor processing.},
  organization = {University of Sussex},
  publisher = {University of Sussex},
  abstract = {Bela is an embedded platform for ultra-low latency audio and sensor processing. We present here the hardware and software features of Bela with particular focus on its integration with Pure Data. Sensor inputs on Bela are sampled at audio rate, which opens to the possibility of doing signal processing using Pure Dataâs audio-rate objects.},
  day = {29},
  finishday = {3},
  finishmonth = {Jul},
  finishyear = {2016},
  keyword = {ultra-low latency audio},
  publicationstatus = {accepted},
  startday = {29},
  startmonth = {Jun},
  startyear = {2016},
  timestamp = {2017.04.05},
  url = {<a href="http://www.liveinterfaces.org/proceedings2016.html">http://www.liveinterfaces.org/proceedings2016.html</a>}
}
</pre>

<a name="mcpherson2016designinginstrument"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#mcpherson2016designinginstrument">mcpherson2016designinginstrument</a>,
  title = {Designing for Exploratory Play with a Hackable Digital Musical Instrument},
  author = {MCPHERSON, A and Chamberlain, A and Hazzard, A and McGrath, S and Benford, S},
  booktitle = {ACM Conference on Designing Interactive Systems},
  year = {2016},
  month = {Jun},
  organization = {Brisbane, Australia},
  day = {4},
  doi = {10.1145/2901790.2901831},
  isbn = {978-1-4503-4031-1},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="sulyok2016evolvingcomposer"></a><pre>
@article{<a href="pubs2016_raw.html#sulyok2016evolvingcomposer">sulyok2016evolvingcomposer</a>,
  title = {Evolving the process of a virtual composer},
  author = {Sulyok, C and MCPHERSON, A and Harte, C},
  journal = {Natural Computing},
  year = {2016},
  month = {Jun},
  day = {3},
  doi = {10.1007/s11047-016-9561-6},
  publicationstatus = {online-published},
  publisher = {Springer},
  timestamp = {2017.04.05}
}
</pre>

<a name="heinrichs2016performance-ledapplications"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#heinrichs2016performance-ledapplications">heinrichs2016performance-ledapplications</a>,
  title = {Performance-Led Design of Computationally Generated Audio for Interactive Applications},
  author = {Heinrichs, C and McPherson, A and ACM},
  booktitle = {PROCEEDINGS OF THE TENTH ANNIVERSARY CONFERENCE ON TANGIBLE EMBEDDED AND EMBODIED INTERACTION (TEI16)},
  year = {2016},
  pages = {697--700},
  doi = {10.1145/2839462.2854109},
  keyword = {Embodied sound design},
  publicationstatus = {published},
  timestamp = {2017.04.05},
  url = {<a href="http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&amp;SrcApp=PARTNER_APP\&amp;SrcAuth=LinksAMR\&amp;KeyUT=WOS:000390588700100\&amp;DestLinkType=FullRecord\&amp;DestApp=ALL_WOS\&amp;UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a">http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&SrcApp=PARTNER_APP\&SrcAuth=LinksAMR\&KeyUT=WOS:000390588700100\&DestLinkType=FullRecord\&DestApp=ALL_WOS\&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a</a>}
}
</pre>

<a name="jack2016navigationfeedback"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#jack2016navigationfeedback">jack2016navigationfeedback</a>,
  title = {Navigation of Pitch Space on a Digital Musical Instrument with Dynamic Tactile Feedback},
  author = {Jack, R and Stockman, T and McPherson, A and ACM},
  booktitle = {PROCEEDINGS OF THE TENTH ANNIVERSARY CONFERENCE ON TANGIBLE EMBEDDED AND EMBODIED INTERACTION (TEI16)},
  year = {2016},
  pages = {3--11},
  doi = {10.1145/2839462.2839503},
  keyword = {Tangible User Interfaces},
  publicationstatus = {published},
  timestamp = {2017.04.05},
  url = {<a href="http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&amp;SrcApp=PARTNER_APP\&amp;SrcAuth=LinksAMR\&amp;KeyUT=WOS:000390588700003\&amp;DestLinkType=FullRecord\&amp;DestApp=ALL_WOS\&amp;UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a">http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&SrcApp=PARTNER_APP\&SrcAuth=LinksAMR\&KeyUT=WOS:000390588700003\&DestLinkType=FullRecord\&DestApp=ALL_WOS\&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a</a>}
}
</pre>

<a name="subramanian2016umaguitarra"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#subramanian2016umaguitarra">subramanian2016umaguitarra</a>,
  title = {Uma abordagem baseada em programaÃ§Ã£o linear inteira para a geraÃ§Ã£o de solos de guitarra},
  author = {Subramanian, A and Cunha, N and HERREMANS, D},
  booktitle = {XLVIII SimpÃ³sio Brasileiro de Pesquisa Operacional (SBPO)},
  year = {2016},
  month = {Sep},
  organization = {Vitoria, Brasil},
  day = {27},
  finishday = {30},
  finishmonth = {Sep},
  finishyear = {2016},
  publicationstatus = {published},
  startday = {27},
  startmonth = {Sep},
  startyear = {2016},
  timestamp = {2017.04.05}
}
</pre>

<a name="agres2016themusic"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#agres2016themusic">agres2016themusic</a>,
  title = {The Effect of Repetitive Structure on Enjoyment in Uplifting Trance Music},
  author = {Agres, K and HERREMANS, D and Bigo, L and Conklin, D},
  booktitle = {14th International Conference for Music Perception and Cognition (ICMPC)},
  year = {2016},
  month = {Jun},
  pages = {280--282},
  day = {5},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="theodorou2016exploringperformances"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#theodorou2016exploringperformances">theodorou2016exploringperformances</a>,
  title = {Exploring audience behaviour during contemporary dance performances},
  author = {Theodorou, L and Healey, PGT and Smeraldi, F},
  booktitle = {ACM International Conference Proceeding Series},
  year = {2016},
  month = {Jul},
  volume = {05-06-July-2016},
  abstract = {How can performers detect and potentially respond to the reactions of a live audience? Audience members' physical movements provide one possible source of information about their engagement with a performance. Using a case study of the dance performance "Frames" that took place in Theatre Royal in Glasgow during March 2015, we examine patterns of audience movement during contemporary dance performances and explore how they relate to the dancer's movements. Video recordings of performers and audience were analysed using computer vision and data analysis techniques extracting facial expression, hand gestural and body movement data. We found that during the performance audiences move very little and have predominantly expressionless faces while hand gestures seem to play a significant role in the way audiences respond. This suggests that stillness i.e. the absence of motion may be an indicator of engagement.},
  day = {5},
  doi = {10.1145/2948910.2948928},
  isbn = {9781450343077},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="healey2016betterphenomena"></a><pre>
@article{<a href="pubs2016_raw.html#healey2016betterphenomena">healey2016betterphenomena</a>,
  title = {Better late than Now-or-Never: The case of interactive repair phenomena},
  author = {HEALEY, PGT and HOWES, C and HOUGH, J and PURVER, MRJ},
  journal = {Behavioral and Brain Sciences},
  year = {2016},
  month = {Jan},
  pages = {e76--e76},
  volume = {39},
  abstract = {Empirical evidence from dialogue, both corpus and experimental, highlights the importance of interaction in language use â and this raises some questions for Christiansen \& Chaterâs (C\&Câs) proposals. We endorse C\&Câs call for an integrated framework but argue that their emphasis on local, individual production and comprehension makes it difficult to accommodate the ubiquitous, interactive, and defeasible processes of clarification and repair in conversation.},
  day = {1},
  doi = {10.1017/S0140525X15000813},
  issn = {1469-1825},
  publicationstatus = {published},
  publisher = {Cambridge University Press (CUP): STM Journals},
  timestamp = {2017.04.05},
  url = {<a href="http://www.eecs.qmul.ac.uk/~mpurver/papers/healey-et-al16bbs.pdf">http://www.eecs.qmul.ac.uk/~mpurver/papers/healey-et-al16bbs.pdf</a>}
}
</pre>

<a name="ewert2016pianoframework"></a><pre>
@article{<a href="pubs2016_raw.html#ewert2016pianoframework">ewert2016pianoframework</a>,
  title = {Piano transcription in the studio using an extensible alternating directions framework},
  author = {Ewert, S and Sandler, M},
  journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
  year = {2016},
  month = {Nov},
  pages = {1983--1997},
  volume = {24},
  abstract = {Â© 2016 IEEE.Given a musical audio recording, the goal of automatic music transcription is to determine a score-like representation of the piece underlying the recording. Despite significant interest within the research community, several studies have reported on a 'glass ceiling' effect, an apparent limit on the transcription accuracy that current methods seem incapable of overcoming. In this paper, we explore how much this effect can be mitigated by focusing on a specific instrument class and making use of additional information on the recording conditions available in studio or home recording scenarios. In particular, exploiting the availability of single note recordings for the instrument in use, we develop a novel signal model employing variable-length spectro-temporal patterns as its central building blocks - tailored for pitched percussive instruments such as the piano. Temporal dependencies between spectral templates are modeled, resembling characteristics of factorial scaled hidden Markov models (FS-HMM) and other methods combining nonnegative matrix factorization with Markov processes. In contrast to FS-HMMs, our parameter estimation is developed in a global, relaxed form within the extensible alternating direction method of multipliers framework, which enables the systematic combination of basic regularizers propagating sparsity and local stationarity in note activity with more complex regularizers imposing temporal semantics. The proposed method achieves an f -measure of 93-95\% for note onsets on pieces recorded on a Yamaha Disklavier (MAPS DB).},
  day = {1},
  doi = {10.1109/TASLP.2016.2593801},
  issn = {2329-9290},
  issue = {11},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="ewert2016representationmethods"></a><pre>
@article{<a href="pubs2016_raw.html#ewert2016representationmethods">ewert2016representationmethods</a>,
  title = {Representation of Musical Structure for a Computationally Feasible Integration with Audio-Based Methods},
  author = {Ewert, S},
  journal = {Dagstuhl Reports (Computational Music Structure Analysis (Dagstuhl Seminar 16092))},
  year = {2016},
  month = {Oct},
  note = {urn: urn:nbn:de:0030-drops-61415},
  number = {2},
  pages = {175--176},
  volume = {6},
  abstract = {In terms of terminology, âmusical structureâ has been used in several, different contexts. In one interpretation, musical structure is essentially equivalent to musical form, which can be considered as a genre or rather style specific definition of the expectation of how a piece is composed on a rather global level. Another interpretation of structure is closer to the corresponding mathematical notion, where structure yields properties and regularity.},
  address = {Dagstuhl, Germany},
  day = {7},
  doi = {10.4230/DagRep.6.2.147},
  editor = {MÃ¼ller, M and Chew, E and Bello, JP},
  issn = {2192-5283},
  issue = {2},
  publicationstatus = {published},
  publisher = {Schloss DagstuhlâLeibniz-Zentrum fuer Informatik},
  timestamp = {2017.04.05},
  url = {<a href="http://drops.dagstuhl.de/opus/volltexte/2016/6141">http://drops.dagstuhl.de/opus/volltexte/2016/6141</a>}
}
</pre>

<a name="driedger2016template-basedsignals"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#driedger2016template-basedsignals">driedger2016template-basedsignals</a>,
  title = {Template-Based Vibrato Analysis in Music Signals},
  author = {Driedger, J and Balke, S and Ewert, S and MÃ¼ller, M},
  booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
  year = {2016},
  address = {New York, USA},
  pages = {239--245},
  timestamp = {2017.04.05}
}
</pre>

<a name="ewert2016score-informedrecordings"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#ewert2016score-informedrecordings">ewert2016score-informedrecordings</a>,
  title = {Score-Informed Identification of Missing and Extra Notes in Piano Recordings},
  author = {Ewert, S and Wang, S and MÃ¼ller, M and Sandler, M},
  booktitle = {Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)},
  year = {2016},
  address = {New York, USA},
  pages = {30--36},
  timestamp = {2017.04.05}
}
</pre>

<a name="mehrabi2016towardssounds"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#mehrabi2016towardssounds">mehrabi2016towardssounds</a>,
  title = {Towards a comprehensive dataset of vocal imitations of drum sounds},
  author = {MEHRABI, A and Dixon, S and Sandler, M},
  booktitle = {2nd AES Workshop on Intelligent Music Production},
  year = {2016},
  month = {Sep},
  day = {12},
  publicationstatus = {online-published},
  timestamp = {2017.04.05}
}
</pre>

<a name="panteli2016learningmusic"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#panteli2016learningmusic">panteli2016learningmusic</a>,
  title = {Learning a feature space for similarity in world music},
  author = {Panteli, M and Benetos, E and Dixon, S},
  booktitle = {17th International Society for Music Information Retrieval Conference},
  year = {2016},
  month = {Aug},
  organization = {New York, USA},
  pages = {538--544},
  publisher = {ISMIR},
  abstract = {In this study we investigate computational methods for assessing music similarity in world music styles. We use state-of-the-art audio features to describe musical content in world music recordings. Our music collection is a subset of the Smithsonian Folkways Recordings with audio examples from 31 countries from around the world. Using supervised and unsupervised dimensionality reduction techniques we learn feature representations for music similarity. We evaluate how well music styles separate in this learned space with a classification experiment. We obtained moderate performance classifying the recordings by country. Analysis of misclassifications revealed cases of geographical or cultural proximity. We further evaluate the learned space by detecting outliers, i.e. identifying recordings that stand out in the collection. We use a data mining technique based on Mahalanobis distances to detect outliers and perform a listening experiment in the âodd one outâ style to evaluate our findings. We are able to detect, amongst others, recordings of non-musical content as outliers as well as music with distinct timbral and harmonic content. The listening experiment reveals moderate agreement between subjectsâ ratings and our outlier estimation.},
  day = {7},
  finishday = {11},
  finishmonth = {Aug},
  finishyear = {2016},
  publicationstatus = {published},
  startday = {7},
  startmonth = {Aug},
  startyear = {2016},
  timestamp = {2017.04.05},
  url = {<a href="http://www.eecs.qmul.ac.uk/~mp305/">http://www.eecs.qmul.ac.uk/~mp305/</a>}
}
</pre>

<a name="cheng2016antranscription"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#cheng2016antranscription">cheng2016antranscription</a>,
  title = {An attack/decay model for piano transcription},
  author = {Cheng, T and Mauch, M and Benetos, E and Dixon, S},
  booktitle = {17th International Society for Music Information Retrieval Conference},
  year = {2016},
  month = {Aug},
  organization = {New York, USA},
  pages = {584--590},
  publisher = {ISMIR},
  abstract = {We demonstrate that piano transcription performance for a known piano can be improved by explicitly modelling piano acoustical features. The proposed method is based on non-negative matrix factorisation, with the following three refinements: (1) introduction of attack and harmonic decay components; (2) use of a spike-shaped note activation that is shared by these components; (3) modelling the harmonic decay with an exponential function. Transcription is performed in a supervised way, with the training and test datasets produced by the same piano. First we train parameters for the attack and decay components on isolated notes, then update only the note activations for transcription. Experiments show that the proposed model achieves 82\% on note-wise and 79\% on frame-wise F-measures on the âENSTDkClâ subset of the MAPS database, outperforming the current published state of the art.},
  day = {7},
  finishday = {11},
  finishmonth = {Aug},
  finishyear = {2016},
  publicationstatus = {published},
  startday = {7},
  startmonth = {Aug},
  startyear = {2016},
  timestamp = {2017.04.05},
  url = {https://wp.nyu.edu/ismir2016/}
}
</pre>

<a name="quinton2016estimationdescriptor"></a><pre>
@article{<a href="pubs2016_raw.html#quinton2016estimationdescriptor">quinton2016estimationdescriptor</a>,
  title = {ESTIMATION OF THE RELIABILITY OF MULTIPLE RHYTHM FEATURES EXTRACTION FROM A SINGLE DESCRIPTOR},
  author = {QUINTON, E and Sandler, M and Dixon, S},
  journal = {IEEE International Conference on Acoustics Speech and Signal Processing},
  year = {2016},
  month = {Apr},
  day = {1},
  issn = {1520-6149},
  keyword = {Rhythm},
  publicationstatus = {published},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  timestamp = {2017.04.05}
}
</pre>

<a name="wang2016robustperformances"></a><pre>
@article{<a href="pubs2016_raw.html#wang2016robustperformances">wang2016robustperformances</a>,
  title = {Robust and efficient joint alignment of multiple musical performances},
  author = {Wang, S and Ewert, S and Dixon, S},
  journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
  year = {2016},
  month = {Nov},
  pages = {2132--2145},
  volume = {24},
  abstract = {Â© 2014 IEEE.The goal of music alignment is to map each temporal position in one version of a piece of music to the corresponding positions in other versions of the same piece. Despite considerable improvements in recent years, state-of-the-art methods still often fail to identify a correct alignment if versions differ substantially with respect to acoustic conditions or musical interpretation. To increase the robustness for these cases, we exploit in this work the availability of multiple versions of the piece to be aligned. By processing these jointly, we can supply the alignment process with additional examples of how a section might be interpreted or which acoustic conditions may arise. This way, we can use alignment information between two versions transitively to stabilize the alignment with a third version. Extending our previous work [1], we present two such joint alignment methods, progressive alignment and probabilistic profile, and discuss their fundamental differences and similarities on an algorithmic level. Our systematic experiments using 376 recordings of 9 pieces demonstrate that both methods can indeed improve the alignment accuracy and robustness over comparable pairwise methods. Further, we provide an in-depth analysis of the behavior of both joint alignment methods, studying the influence of parameters such as the number of performances available, comparing their computational costs, and investigating further strategies to increase both.},
  day = {1},
  doi = {10.1109/TASLP.2016.2598318},
  issn = {2329-9290},
  issue = {11},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="song2016perceivedmodels"></a><pre>
@article{<a href="pubs2016_raw.html#song2016perceivedmodels">song2016perceivedmodels</a>,
  title = {Perceived and Induced Emotion Responses to Popular Music: Categorical and Dimensional Models},
  author = {Song, Y and Dixon, S and Pearce, MT and Halpern, AR},
  journal = {Music Perception},
  year = {2016},
  pages = {472--492},
  volume = {33},
  abstract = {Music both conveys and evokes emotions ,
and although both phenomena are widely studied, the
difference between them is often neglected. The purpose
of this study is to examine the difference between
perceived and induced emotion for Western popular
music using both categorical and dimensional models
of emotion, and to examine the influence of individual
listener differences on their emotion judgment. A total
of 80 musical excerpts were randomly selected from an
established dataset of 2,904 popular songs tagged with
one of the four words ââhappy,ââ ââsad,ââ ââangry,ââ or
âârelaxedââ on the Last.FM web site. Participants listened
to the excerpts and rated perceived and induced emotion
on the categorical model and dimensional model, and
the reliability of emotion tags was evaluated according
to participantsâ agreement with corresponding labels. In
addition, the Goldsmiths Musical Sophistication Index
(Gold-MSI) was used to assess participantsâ musical
expertise and engagement. As expected, regardless of
the emotion model used, music evokes emotions similar
to the emotional quality perceived in music. Moreover,
emotion tags predict music emotion judgments. How-
ever, age, gender and three factors from Gold-MSI,
importance, emotion, and music training were found not
to predict listenersâ responses, nor the agreement with
tags.},
  doi = {10.1525/MP.2016.33.4.472},
  issue = {4},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="herremans2016morpheus:profiles"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#herremans2016morpheus:profiles">herremans2016morpheus:profiles</a>,
  title = {MorpheuS: Automatic music generation with recurrent pattern constraints and tension profiles},
  author = {HERREMANS, D and Chew, E},
  booktitle = {IEEE TENCON},
  year = {2016},
  month = {Nov},
  organization = {Singapore},
  day = {22},
  finishday = {25},
  finishmonth = {Nov},
  finishyear = {2016},
  publicationstatus = {published},
  startday = {22},
  startmonth = {Nov},
  startyear = {2016},
  timestamp = {2017.04.05}
}
</pre>

<a name="yang2016ava:analysis"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#yang2016ava:analysis">yang2016ava:analysis</a>,
  title = {AVA: A Graphical User Interface for Automatic Vibrato and Portamento Detection and Analysis},
  author = {YANG, L and Rajab, SAYID-KHALID and Chew, E},
  booktitle = {the 42nd International Computer Music Conference},
  year = {2016},
  month = {Sep},
  organization = {Utrecht, The Netherlands},
  day = {12},
  finishday = {16},
  finishmonth = {Sep},
  finishyear = {2016},
  publicationstatus = {published},
  startday = {12},
  startmonth = {Sep},
  startyear = {2016},
  timestamp = {2017.04.05}
}
</pre>

<a name="volk2016musiccomputation"></a><pre>
@article{<a href="pubs2016_raw.html#volk2016musiccomputation">volk2016musiccomputation</a>,
  title = {Music Similarity: Concepts, Cognition and Computation},
  author = {Volk, A and Chew, E and Margulis, EH and Anagnostopoulou, C},
  journal = {JOURNAL OF NEW MUSIC RESEARCH},
  year = {2016},
  month = {Sep},
  pages = {207--209},
  volume = {45},
  doi = {10.1080/09298215.2016.1232412},
  eissn = {1744-5027},
  issn = {0929-8215},
  issue = {3},
  publicationstatus = {published},
  timestamp = {2017.04.05},
  url = {<a href="http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&amp;SrcApp=PARTNER_APP\&amp;SrcAuth=LinksAMR\&amp;KeyUT=WOS:000385541200001\&amp;DestLinkType=FullRecord\&amp;DestApp=ALL_WOS\&amp;UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a">http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&SrcApp=PARTNER_APP\&SrcAuth=LinksAMR\&KeyUT=WOS:000385541200001\&DestLinkType=FullRecord\&DestApp=ALL_WOS\&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a</a>}
}
</pre>

<a name="kosta2016mappingapproach"></a><pre>
@article{<a href="pubs2016_raw.html#kosta2016mappingapproach">kosta2016mappingapproach</a>,
  title = {Mapping between dynamic markings and performed loudness: A machine learning approach},
  author = {Kosta, K and Ramirez, R and Bandtlow, OF and Chew, E},
  journal = {Journal of Mathematics and Music},
  year = {2016},
  month = {Aug},
  pages = {149--172},
  volume = {10},
  abstract = {Loudness variation is one of the foremost tools for expressivity in music performance. Loudness is frequently notated as dynamic markings such as "p" (piano, meaning soft) or "f" (forte, meaning loud). While dynamic markings in music scores are important indicators of how music pieces should be interpreted, their meaning is less straightforward than it may seem, and depends highly on the context in which they appear. In this article, we investigate the relationship between dynamic markings in the score and performed loudness by applying machine learning techniques â decision trees, support vector machines, artificial neural networks, and a k-nearest neighbor method â to the prediction of loudness levels corresponding to dynamic markings, and to the classification of dynamic markings given loudness values. The methods are applied to 44 recordings of performances of Chopin's Mazurkas, each by 8 pianists. The results show that loudness values and markings can be predicted relatively well when trained across recordings of the same piece, but fail dismally when trained across the pianist's recordings of other pieces, demonstrating that score features may trump individual style when modeling loudness choices. Evidence suggests that all the features chosen for the task are relevant, and analysis of the results reveals the forms (such as the return of the theme) and structures (such as dynamic-marking repetitions) that influence the predictability of loudness and markings. Modeling of loudness trends in expressive performance appears to be a delicate matter, and sometimes loudness expression can be a matter of the performer's idiosyncracy.},
  day = {3},
  editor = {Fiore, T},
  issn = {1745-9745},
  keyword = {dynamic markings},
  publicationstatus = {published},
  publisher = {Taylor \&amp; Francis: STM, Behavioural Science and Public Health Titles},
  timestamp = {2017.04.05}
}
</pre>

<a name="herremans2016tensiontension."></a><pre>
@inproceedings{<a href="pubs2016_raw.html#herremans2016tensiontension.">herremans2016tensiontension.</a>,
  title = {Tension ribbons: Quantifying and visualising tonal tension.},
  author = {HERREMANS, D and Chew, E},
  booktitle = {Second International Conference on Technologies for Music Notation and Representation},
  year = {2016},
  month = {May},
  organization = {Cambridge},
  pages = {8--18},
  volume = {2},
  day = {15},
  finishday = {27},
  finishmonth = {May},
  finishyear = {2016},
  publicationstatus = {published},
  startday = {29},
  startmonth = {May},
  startyear = {2016},
  timestamp = {2017.04.05}
}
</pre>

<a name="chew2016playingtonality"></a><pre>
@article{<a href="pubs2016_raw.html#chew2016playingtonality">chew2016playingtonality</a>,
  title = {PLAYING WITH THE EDGE: TIPPING POINTS AND THE ROLE OF TONALITY},
  author = {Chew, E},
  journal = {MUSIC PERCEPTION},
  year = {2016},
  month = {Feb},
  pages = {344--366},
  volume = {33},
  doi = {10.1525/MP.2016.33.03.344},
  issn = {0730-7829},
  issue = {3},
  keyword = {thresholds},
  publicationstatus = {published},
  timestamp = {2017.04.05},
  url = {<a href="http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&amp;SrcApp=PARTNER_APP\&amp;SrcAuth=LinksAMR\&amp;KeyUT=WOS:000373749600007\&amp;DestLinkType=FullRecord\&amp;DestApp=ALL_WOS\&amp;UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a">http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&SrcApp=PARTNER_APP\&SrcAuth=LinksAMR\&KeyUT=WOS:000373749600007\&DestLinkType=FullRecord\&DestApp=ALL_WOS\&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a</a>}
}
</pre>

<a name="metatla2016audio-hapticapproach"></a><pre>
@article{<a href="pubs2016_raw.html#metatla2016audio-hapticapproach">metatla2016audio-hapticapproach</a>,
  title = {Audio-haptic interfaces for digital audio workstations: A participatory design approach},
  author = {Metatla, O and Martin, F and Parkinson, A and Bryan-Kinns, N and Stockman, T and Tanaka, A},
  journal = {Journal on Multimodal User Interfaces},
  year = {2016},
  month = {Sep},
  pages = {247--258},
  volume = {10},
  abstract = {Â© 2016, The Author(s).We examine how auditory displays, sonification and haptic interaction design can support visually impaired sound engineers, musicians and audio production specialists access to digital audio workstation. We describe a user-centred approach that incorporates various participatory design techniques to help make the design process accessible to this population of users. We also outline the audio-haptic designs that results from this process and reflect on the benefits and challenges that we encountered when applying these techniques in the context of designing support for audio editing.},
  day = {1},
  doi = {10.1007/s12193-016-0217-8},
  eissn = {1783-8738},
  issn = {1783-8738},
  issue = {3},
  publicationstatus = {published},
  publisher = {Springer Verlag (Germany)},
  timestamp = {2017.04.05}
}
</pre>

<a name="metatla2016tapinterface"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#metatla2016tapinterface">metatla2016tapinterface</a>,
  title = {Tap the ShapeTones: Exploring the Effects of Crossmodal Congruence in an Audio-Visual Interface},
  author = {Metatla, O and Correia, N and Martin, F and Bryan-Kinns, N and Stockman, T},
  year = {2016},
  note = {proceedings: Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems
},
  pages = {1055--1066},
  timestamp = {2017.04.05}
}
</pre>

<a name="mazzoni2016moody:music"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#mazzoni2016moody:music">mazzoni2016moody:music</a>,
  title = {Moody: Haptic sensations to enhance mood in film music},
  author = {Mazzoni, A and Bryan-Kinns, N},
  booktitle = {DIS 2016 Companion - Proceedings of the 2016 ACM Conference on Designing Interactive Systems: Fuse},
  year = {2016},
  month = {Jun},
  pages = {21--24},
  abstract = {In this paper we present Moody, a haptic wearable prototype intended to enhance mood music in Film through haptic sensations. Moody is the design resulting from an exploratory study aimed at finding new ways to enrich emotions in film entertainment. The work is aimed at designing expressive haptic patterns to heighten suspense and excitement in those movie scenes where the music score amplifies what can't be shown, and that, is the mood.},
  day = {6},
  doi = {10.1145/2908805.2908811},
  isbn = {9781450343152},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="benetos2016automaticquartets"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#benetos2016automaticquartets">benetos2016automaticquartets</a>,
  title = {Automatic Transcription of Vocal Quartets},
  author = {BENETOS, E and Schramm,, R},
  booktitle = {DMRN+11: Digital Music Research Network Workshop Proceedings 2016},
  year = {2016},
  month = {Dec},
  organization = {Centre for Digital Music, Queen Mary University of London},
  publisher = {Centre for Digital Music, Queen Mary University of London},
  abstract = {This work presents a probabilistic latent component analysis (PLCA) method applied to automatic music transcription of a cappella performances of vocal quartets. A variable-Q transform (VQT) representation of the audio spectrogram is factorised with the help of a 6-dimensional tensor. Preliminary experiments have shown promising music transcription results when applied to audio recordings of Bach Chorales and Barbershop music.},
  day = {20},
  finishday = {20},
  finishmonth = {Dec},
  finishyear = {2016},
  publicationstatus = {published},
  startday = {20},
  startmonth = {Dec},
  startyear = {2016},
  timestamp = {2017.04.05},
  url = {https://qmro.qmul.ac.uk/xmlui/handle/123456789/19345}
}
</pre>

<a name="lafay2016adetection"></a><pre>
@article{<a href="pubs2016_raw.html#lafay2016adetection">lafay2016adetection</a>,
  title = {A morphological model for simulating acoustic scenes and its application to sound event detection},
  author = {Lafay, G and Lagrange, M and Rossignol, M and Benetos, E and Roebel, A},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year = {2016},
  month = {Oct},
  pages = {1854--1864},
  volume = {24},
  abstract = {This paper introduces a model for simulating environmental acoustic scenes that abstracts temporal structures from audio recordings. This model allows us to explicitly control key morphological aspects of the acoustic scene and to isolate their impact on the performance of the system under evaluation. Thus, more information can be gained on the behavior of an evaluated system, providing guidance for further improvements. To demonstrate its potential, this model is employed to evaluate the performance of nine state of the art sound event detection systems submitted to the IEEE DCASE 2013 Challenge. Results indicate that the proposed scheme is able to successfully build datasets useful for evaluating important aspects of the performance of sound event detection systems, such as their robustness to new recording conditions and to varying levels of background audio.},
  day = {1},
  doi = {10.1109/TASLP.2016.2587218},
  issue = {10},
  publicationstatus = {published},
  publisher = {IEEE},
  timestamp = {2017.04.05},
  url = {<a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7503122">http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7503122</a>}
}
</pre>

<a name="valero-mas2016classification-basedtranscription"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#valero-mas2016classification-basedtranscription">valero-mas2016classification-basedtranscription</a>,
  title = {Classification-based Note Tracking for Automatic Music Transcription},
  author = {Valero-Mas, JJ and Benetos, E and IÃ±esta, JM},
  booktitle = {https://sites.google.com/site/musicmachinelearning16/proceedings},
  year = {2016},
  month = {Sep},
  organization = {Riva del Garda, Italy},
  pages = {61--65},
  abstract = {Note tracking constitutes a key process in Automatic Music Transcription as it derives a note-level transcription from a frame-based pitch activation representation. While this stage is commonly performed using a set of hand-crafted rules, this work presents an approach based on supervised classification which automatically infers these policies. An initial frame-level estimation provides the necessary information for segmenting each pitch band in single instances which are later classified as active or non-active note events. Preliminary results using classic classification strategies on a subset of the MAPS piano dataset report an improvement of up to a 15\% when compared to the baseline considered for both frame-level and note-level assessment.},
  day = {23},
  finishday = {23},
  finishmonth = {Sep},
  finishyear = {2016},
  publicationstatus = {published},
  startday = {23},
  startmonth = {Sep},
  startyear = {2016},
  timestamp = {2017.04.05},
  url = {https://sites.google.com/site/musicmachinelearning16/}
}
</pre>

<a name="abdallah2016digitaldata"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#abdallah2016digitaldata">abdallah2016digitaldata</a>,
  title = {Digital Music Lab: A Framework for Analysing Big Music Data},
  author = {Abdallah, S and Benetos, E and Gold, N and Hargreaves, S and Weyde, T and Wolff, D},
  booktitle = {24th European Signal Processing Conference},
  year = {2016},
  month = {Aug},
  organization = {Budapest, Hungary},
  pages = {1118--1122},
  publisher = {EURASIP},
  day = {29},
  finishday = {2},
  finishmonth = {Sep},
  finishyear = {2016},
  publicationstatus = {published},
  startday = {29},
  startmonth = {Aug},
  startyear = {2016},
  timestamp = {2017.04.05}
}
</pre>

<a name="holzapfel2016thetunes"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#holzapfel2016thetunes">holzapfel2016thetunes</a>,
  title = {The Sousta corpus: Beat-informed automatic transcription of traditional dance tunes},
  author = {Holzapfel, A and Benetos, E},
  booktitle = {17th International Society for Music Information Retrieval Conference},
  year = {2016},
  month = {Aug},
  organization = {New York, USA},
  pages = {531--537},
  publisher = {ISMIR},
  abstract = {In this paper, we present a new corpus for research in computational ethnomusicology and automatic music transcription, consisting of traditional dance tunes from Crete. This rich dataset includes audio recordings, scores transcribed by ethnomusicologists and aligned to the audio performances, and meter annotations. A second contribution of this paper is the creation of an automatic music transcription system able to support the detection of multiple pitches produced by lyra (a bowed string instrument). Furthermore, the transcription system is able to cope with deviations from standard tuning, and provides temporally quantized notes by combining the output of the multi-pitch detection stage with a state-of-the-art meter tracking algorithm. Experiments carried out for note tracking using 25ms onset tolerance reach 41.1\% using information from the multi-pitch detection stage only, 54.6\% when integrating beat information, and 57.9\% when also supporting tuning estimation. The produced meter aligned transcriptions can be used to generate staff notation, a fact that increases the value of the system for studies in ethnomusicology.},
  day = {7},
  finishday = {11},
  finishmonth = {Aug},
  finishyear = {2016},
  publicationstatus = {published},
  startday = {7},
  startmonth = {Aug},
  startyear = {2016},
  timestamp = {2017.04.05},
  url = {https://wp.nyu.edu/ismir2016/}
}
</pre>

<a name="panteli2016automaticcollections"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#panteli2016automaticcollections">panteli2016automaticcollections</a>,
  title = {Automatic detection of outliers in world music collections},
  author = {Panteli, M and Benetos, E and Dixon, S},
  booktitle = {Fourth International Conference on Analytical Approaches to World Music (AAWM 2016)},
  year = {2016},
  address = {Maria Panteli, Queen Mary University of London, School of Electronic Enginering and Computer Science, Mile End Road, London, E1 4NS, United Kingdom},
  month = {Jun},
  organization = {New York, USA},
  day = {8},
  finishday = {8},
  finishmonth = {Jun},
  finishyear = {2016},
  publicationstatus = {accepted},
  startday = {11},
  startmonth = {Jun},
  startyear = {2016},
  timestamp = {2017.04.05},
  url = {<a href="http://www.eecs.qmul.ac.uk/~mp305/">http://www.eecs.qmul.ac.uk/~mp305/</a>}
}
</pre>

<a name="benetos2016detectionmodel"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#benetos2016detectionmodel">benetos2016detectionmodel</a>,
  title = {Detection of overlapping acoustic events using a temporally-constrained probabilistic model},
  author = {Benetos, E and Lafay, G and Lagrange, M and Plumbley, MD},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},
  year = {2016},
  address = {Emmanouil Benetos, Queen Mary University of London, School of Electronic Engineering and Computer Science, Mile End Road, London, E1 4NS, United Kingdom},
  month = {Mar},
  organization = {Shanghai, China},
  pages = {6450--6454},
  publisher = {IEEE},
  abstract = {In this paper, a system for overlapping acoustic event detection is proposed, which models the temporal evolution of sound events. The system is based on probabilistic latent component analysis, supporting the use of a sound event dictionary where each exemplar consists of a succession of spectral templates. The temporal succession of the templates is controlled through event class-wise Hidden Markov Models (HMMs). As input time/frequency representation, the Equivalent Rectangular Bandwidth (ERB) spectrogram is used. Experiments are carried out on polyphonic datasets of office sounds generated using an acoustic scene simulator, as well as real and synthesized monophonic datasets for comparative purposes. Results show that the proposed system outperforms several state-of-the-art methods for overlapping acoustic event detection on the same task, using both frame-based and event-based metrics, and is robust to varying event density and noise levels.},
  day = {20},
  doi = {10.1109/ICASSP.2016.7472919},
  finishday = {25},
  finishmonth = {Mar},
  finishyear = {2016},
  keyword = {Acoustic event detection},
  publicationstatus = {published},
  startday = {20},
  startmonth = {Mar},
  startyear = {2016},
  timestamp = {2017.04.05},
  url = {<a href="http://www.eecs.qmul.ac.uk/~emmanouilb/index.html">http://www.eecs.qmul.ac.uk/~emmanouilb/index.html</a>}
}
</pre>

<a name="zhang2016acase"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#zhang2016acase">zhang2016acase</a>,
  title = {A Web Application for Audience Participation in Live Music Performance: The Open Symphony Use Case},
  author = {Zhang, L and Wu, Y and BARTHET, M},
  booktitle = {International Conference on New Interfaces for Musical Expression},
  year = {2016},
  month = {Jul},
  abstract = {This paper presents a web-based application enabling au-
diences to collaboratively contribute to the creative pro-
cess during live music performances. The system aims at
enhancing audience engagement and creating new forms
of live music experiences. Interaction between audience
and performers is made possible through a client/server
architecture enabling bidirectional communication of cre-
ative data. Audience members can vote for pre-determined
musical attributes using a smartphone-friendly and cross-
platform web application. The system gathers audience
members' votes and provide feedback through visualisations
that can be tailored for speci c needs. In order to sup-
port multiple performers and large audiences, automatic
audience-to-performer groupings are handled by the appli-
cation. The framework was applied to support live interac-
tive musical improvisations where creative roles are shared
amongst audience and performers (Open Symphony). Qual-
itative analyses of user surveys highlighted very positive
feedback related to themes such as engagement and cre-
ativity and also identi ed further design challenges around
audience sense of control and latency.},
  day = {11},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="barthet2016crossroads:listening"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#barthet2016crossroads:listening">barthet2016crossroads:listening</a>,
  title = {Crossroads: Interactive Music Systems Transforming Performance, Production and Listening},
  author = {BARTHET, M and Thalmann, F and Fazekas, G and Sandler, M and Wiggins, G},
  booktitle = {ACM Conference on Human Factors in Computing Systems (CHI), Workshop on Music and HCI},
  year = {2016},
  month = {May},
  abstract = {We discuss several state-of-the-art systems that propose
new paradigms and user workflows for music composition,
production, performance, and listening. We focus on a selection
of systems that exploit recent advances in semantic
and affective computing, music information retrieval (MIR)
and semantic web, as well as insights from fields such as
mobile computing and information visualisation. These systems
offer the potential to provide transformative experiences
for users, which is manifested in creativity, engagement,
efficiency, discovery and affect.},
  day = {7},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="hayes2016asystem"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#hayes2016asystem">hayes2016asystem</a>,
  title = {A Participatory Live Music Performance with the Open Symphony System},
  author = {Hayes, K and BARTHET, M and Wu, Y and Zhang, L and Bryan-Kinns, N},
  booktitle = {ACM Conference on Human Factors in Computing Systems (CHI): Interactivity},
  year = {2016},
  month = {May},
  abstract = {Our Open Symphony system reimagines the music
experience for a digital age, fostering alliances between
performer and audience and our digital selves. Open
Symphony enables live participatory music performance
where the audience actively engages in the music
creation process. This is made possible by using stateof-
the-art web technologies and data visualisation
techniques. Through collaborations with local
performers we will conduct a series of interactive music
performance revolutionizing the performance
experience both for performers and audiences. The
system throws open music-creating possibilities to
every participant and is a genuine novel way to
demonstrate the field of Human Computer Interaction
through computer-supported cooperative creation and
multimodal music and visual perception.},
  day = {7},
  doi = {10.1145/2851581.2889471},
  publicationstatus = {published},
  timestamp = {2017.04.05}
}
</pre>

<a name="wilkinson2016performablemapping"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#wilkinson2016performablemapping">wilkinson2016performablemapping</a>,
  title = {Performable Spectral Synthesis via Low-Dimensional Modelling and
Control Mapping},
  author = {WILKINSON, WJ and stowell, D and reiss, J},
  booktitle = {DMRN+11: Digital Music Research Network Workshop Proceedings 2016},
  year = {2016},
  month = {Dec},
  organization = {Centre for Digital Music, Queen Mary University of London},
  publisher = {Centre for Digital Music, Queen Mary University of London},
  abstract = {Spectral modelling represents an audio signal as
the sum of a finite number of partials â sinusoids tracked
through sequential analysis frames. With the goal of real-time
user-controllable synthesis in mind, we assume these observed
partials to be correlated functions of time, and that there exists
some lower-dimensional set of unobserved forcing functions
driving the partials through a set of differential equations.
Mapping of these unobserved functions to a user control space
provides us with a hybrid approach to synthesis in which
mechanistic controls are exposed to the user but the systemâs
behavioural response to these mechanisms is learnt from data},
  day = {20},
  finishday = {20},
  finishmonth = {Dec},
  finishyear = {2016},
  publicationstatus = {published},
  startday = {20},
  startmonth = {Dec},
  startyear = {2016},
  timestamp = {2017.04.05},
  url = {https://qmro.qmul.ac.uk/xmlui/handle/123456789/19345}
}
</pre>

<a name="alvaradodurangaussiananalysis"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#alvaradodurangaussiananalysis">alvaradodurangaussiananalysis</a>,
  title = {Gaussian Processes for Music Audio Modelling and Content Analysis},
  author = {Alvarado Duran, PA and STOWELL, DF},
  booktitle = {IEEE International Workshop on Machine Learning for Signal Processing (MLSP)},
  year = {2016},
  abstract = {Real music signals are highly variable, yet they have strong statistical
structure. Prior information about the underlying physical mechanisms
by which sounds are generated and rules by which complex sound struc-
ture is constructed (notes, chords, a complete musical score), can be nat-
urally unified using Bayesian modelling techniques. Typically algorithms
for Automatic Music Transcription independently carry out individual
tasks such as multiple-F0 detection and beat tracking. The challenge
remains to perform joint estimation of all parameters. We present a Ba-
yesian approach for modelling music audio, and content analysis. The
proposed methodology based on Gaussian processes seeks joint estima-
tion of multiple music concepts by incorporating into the kernel prior
information about non-stationary behaviour, dynamics, and rich spectral
content present in the modelled music signal. We illustrate the benefits
of this approach via two tasks: pitch estimation, and inferring missing
segments in a polyphonic audio recording.},
  publicationstatus = {accepted},
  timestamp = {2017.04.05}
}
</pre>

<a name="stowellbirdchallenge"></a><pre>
@inproceedings{<a href="pubs2016_raw.html#stowellbirdchallenge">stowellbirdchallenge</a>,
  title = {Bird Detection In Audio: A Survey And A Challenge},
  author = {STOWELL, DF and Wood, M and Stylianou, Y and Glotin, H},
  booktitle = {IEEE International Workshop on Machine Learning for Signal Processing (MLSP)},
  year = {2016},
  abstract = {Many biological monitoring projects rely on acoustic detection of birds. Despite increasingly large datasets, this detection is often manual or semi-automatic, requiring manual tuning/postprocessing. We review the state of the art in automatic bird sound detection, and identify a widespread need for tuning-free and species-agnostic approaches. We introduce new datasets and an IEEE research challenge to address this need, to make possible the development of fully automatic algorithms for bird sound detection.},
  publicationstatus = {accepted},
  timestamp = {2017.04.05}
}
</pre>

<hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.98.</em></p>
