% This file was created with JabRef 2.10b2.
% Encoding: ISO8859_1


@Article{Barascud2016,
  Title                    = {Brain responses in humans reveal ideal observer-like sensitivity to complex acoustic patterns},
  Author                   = {Barascud, N and Pearce, MT and Griffiths, TD and Friston, KJ and Chait, M},
  Journal                  = {PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA},
  Year                     = {2016},

  Month                    = {Feb},
  Pages                    = {E616--E625},
  Volume                   = {113},

  Day                      = {2},
  Doi                      = {10.1073/pnas.1508523113},
  ISSN                     = {0027-8424},
  Issue                    = {5},
  Keyword                  = {statistical learning},
  Owner                    = {dan},
  Publicationstatus        = {published},
  Timestamp                = {2016.04.04},
  Url                      = {http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&SrcApp=PARTNER_APP\&SrcAuth=LinksAMR\&KeyUT=WOS:000369085100018\&DestLinkType=FullRecord\&DestApp=ALL_WOS\&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a}
}

@InProceedings{BENETOS2016,
  Title                    = {Detection of overlapping acoustic events using a temporally-constrained probabilistic model},
  Author                   = {BENETOS, E and Lafay, G and Lagrange, M and Plumbley, MD},
  Year                     = {2016},

  Address                  = {Emmanouil Benetos, Queen Mary University of London, School of Electronic Engineering and Computer Science, Mile End Road, London, E1 4NS, United Kingdom},
  Month                    = {Mar},
  Organization             = {Shanghai, China},
  Pages                    = {6450--6454},
  Publisher                = {IEEE},

  Abstract                 = {In this paper, a system for overlapping acoustic event detection is proposed, which models the temporal evolution of sound events. The system is based on probabilistic latent component analysis, supporting the use of a sound event dictionary where each exemplar consists of a succession of spectral templates. The temporal succession of the templates is controlled through event class-wise Hidden Markov Models (HMMs). As input time/frequency representation, the Equivalent Rectangular Bandwidth (ERB) spectrogram is used. Experiments are carried out on polyphonic datasets of office sounds generated using an acoustic scene simulator, as well as real and synthesized monophonic datasets for comparative purposes. Results show that the proposed system outperforms several state-of-the-art methods for overlapping acoustic event detection on the same task, using both frame-based and event-based metrics, and is robust to varying event density and noise levels.},
  Conference               = {IEEE International Conference on Acoustics, Speech, and Signal Processing},
  Day                      = {20},
  Finishday                = {25},
  Finishmonth              = {Mar},
  Finishyear               = {2016},
  Keyword                  = {Acoustic event detection},
  Owner                    = {dan},
  Publicationstatus        = {accepted},
  Startday                 = {20},
  Startmonth               = {Mar},
  Startyear                = {2016},
  Timestamp                = {2016.04.04},
  Url                      = {http://www.eecs.qmul.ac.uk/~emmanouilb/index.html}
}

@Article{Gingras2016,
  Title                    = {Linking melodic expectation to expressive performance timing and perceived musical tension.},
  Author                   = {Gingras, B and Pearce, MT and Goodchild, M and Dean, RT and Wiggins, G and McAdams, S},
  Journal                  = {Journal of experimental psychology. Human perception and performance},
  Year                     = {2016},

  Month                    = {Apr},
  Pages                    = {594--609},
  Volume                   = {42},

  Abstract                 = {This research explored the relations between the predictability of musical structure, expressive timing in performance, and listeners' perceived musical tension. Studies analyzing the influence of expressive timing on listeners' affective responses have been constrained by the fact that, in most pieces, the notated durations limit performers' interpretive freedom. To circumvent this issue, we focused on the unmeasured prelude, a semi-improvisatory genre without notated durations. In Experiment 1, 12 professional harpsichordists recorded an unmeasured prelude on a harpsichord equipped with a MIDI console. Melodic expectation was assessed using a probabilistic model (IDyOM [Information Dynamics of Music]) whose expectations have been previously shown to match closely those of human listeners. Performance timing information was extracted from the MIDI data using a score-performance matching algorithm. Time-series analyses showed that, in a piece with unspecified note durations, the predictability of melodic structure measurably influenced tempo fluctuations in performance. In Experiment 2, another 10 harpsichordists, 20 nonharpsichordist musicians, and 20 nonmusicians listened to the recordings from Experiment 1 and rated the perceived tension continuously. Granger causality analyses were conducted to investigate predictive relations among melodic expectation, expressive timing, and perceived tension. Although melodic expectation, as modeled by IDyOM, modestly predicted perceived tension for all participant groups, neither of its components, information content or entropy, was Granger causal. In contrast, expressive timing was a strong predictor and was Granger causal. However, because melodic expectation was also predictive of expressive timing, our results outline a complete chain of influence from predictability of melodic structure via expressive performance timing to perceived musical tension. (PsycINFO Database Record},
  Address                  = {Institute of Psychology, University of Innsbruck.},
  Doi                      = {10.1037/xhp0000141},
  Eissn                    = {1939-1277},
  HowPublished             = {Print-Electronic},
  ISSN                     = {0096-1523},
  Issue                    = {4},
  Language                 = {eng},
  Owner                    = {dan},
  Timestamp                = {2016.04.04}
}

@Article{Pearce2016,
  Title                    = {Neuroaesthetics: The Cognitive Neuroscience of Aesthetic Experience.},
  Author                   = {Pearce, MT and Zaidel, DW and Vartanian, O and Skov, M and Leder, H and Chatterjee, A and Nadal, M},
  Journal                  = {Perspectives on psychological science : a journal of the Association for Psychological Science},
  Year                     = {2016},

  Month                    = {Mar},
  Pages                    = {265--279},
  Volume                   = {11},

  Abstract                 = {The field of neuroaesthetics has gained in popularity in recent years but also attracted criticism from the perspectives both of the humanities and the sciences. In an effort to consolidate research in the field, we characterize neuroaesthetics as the cognitive neuroscience of aesthetic experience, drawing on long traditions of research in empirical aesthetics on the one hand and cognitive neuroscience on the other. We clarify the aims and scope of the field, identifying relations among neuroscientific investigations of aesthetics, beauty, and art. The approach we advocate takes as its object of study a wide spectrum of aesthetic experiences, resulting from interactions of individuals, sensory stimuli, and context. Drawing on its parent fields, a cognitive neuroscience of aesthetics would investigate the complex cognitive processes and functional networks of brain regions involved in those experiences without placing a value on them. Thus, the cognitive neuroscientific approach may develop in a way that is mutually complementary to approaches in the humanities.},
  Address                  = {School of Electronic Engineering and Computer Science, Queen Mary University of London, England marcus.pearce@qmul.ac.uk.},
  Doi                      = {10.1177/1745691615621274},
  Eissn                    = {1745-6924},
  HowPublished             = {Print},
  ISSN                     = {1745-6916},
  Issue                    = {2},
  Language                 = {eng},
  Owner                    = {dan},
  Timestamp                = {2016.04.04}
}

@Article{Sigtia2016,
  Title                    = {An End-to-End Neural Network for Polyphonic Piano Music Transcription},
  Author                   = {Sigtia, S and BENETOS, E and Dixon, S},
  Journal                  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  Year                     = {2016},

  Month                    = {May},
  Pages                    = {927--939},
  Volume                   = {24},

  Address                  = {Siddharth Sigtia, Queen Mary University of London, School of Electronic Engineering and Computer Science, Mile End Road, London, E1 4NS, United Kingdom},
  Day                      = {1},
  Doi                      = {10.1109/TASLP.2016.2533858},
  ISSN                     = {2329-9290},
  Issue                    = {5},
  Keyword                  = {Automatic Music Transcription},
  Owner                    = {dan},
  Publicationstatus        = {published},
  Publisher                = {IEEE},
  Timestamp                = {2016.04.04},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7416164}
}

@Article{Song2016,
  Title                    = {Perceived and Induced Emotion Responses to Popular Music: Categorical and Dimensional Models},
  Author                   = {Song, Y and Dixon, S and Pearce, MT and Halpern, AR},
  Journal                  = {Music Perception},
  Year                     = {2016},
  Note                     = {to appear

},

  Owner                    = {dan},
  Timestamp                = {2016.04.04}
}

@InProceedings{THALMANN,
  Title                    = {The Mobile Audio Ontology: Experiencing Dynamic Music Objects on Mobile Devices},
  Author                   = {THALMANN, FS and perez carillo and fazekas and wiggins and sandler},
  Year                     = {2016},

  Abstract                 = {This paper is about the Mobile Audio Ontology, a semantic audio framework for the design of novel music consumption experiences on mobile devices. The framework is based on the concept of the Dynamic Music Object which is an amalgamation of audio files, structural and analytical information extracted from the audio, and information about how it should be rendered in realtime. The Mobile Audio Ontology allows producers and distributors to specify a great variety of ways of playing back music in controlled indeterministic as well as adaptive and interactive ways. Users can map mobile sensor data, user interface controls, or autonomous control units hidden from the listener to any musical parameter exposed in the definition of a Dynamic Music Object. These mappings can also be made dependent on semantic and analytical information extracted from the audio.},
  Conference               = {Tenth IEEE International Conference on Semantic Computing},
  Owner                    = {dan},
  Publicationstatus        = {published},
  Timestamp                = {2016.04.04}
}

@Article{Wang2016,
  Title                    = {Self-Localization of Ad-Hoc Arrays Using Time Difference of Arrivals},
  Author                   = {Wang, L and Hon, T-K and Reiss, JD and Cavallaro, A},
  Journal                  = {IEEE TRANSACTIONS ON SIGNAL PROCESSING},
  Year                     = {2016},

  Month                    = {Feb},
  Pages                    = {1018--1033},
  Volume                   = {64},

  Day                      = {15},
  Doi                      = {10.1109/TSP.2015.2498130},
  ISSN                     = {1053-587X},
  Issue                    = {4},
  Keyword                  = {Ad-hoc array},
  Owner                    = {dan},
  Publicationstatus        = {published},
  Timestamp                = {2016.04.04},
  Url                      = {http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2\&SrcApp=PARTNER_APP\&SrcAuth=LinksAMR\&KeyUT=WOS:000369438500017\&DestLinkType=FullRecord\&DestApp=ALL_WOS\&UsrCustomerID=612ae0d773dcbdba3046f6df545e9f6a}
}

